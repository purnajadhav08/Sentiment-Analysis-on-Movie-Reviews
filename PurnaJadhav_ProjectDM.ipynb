{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Sentiment Analysis on Movie Reviews\n",
    "\n",
    "This project is an individual project. In this project, you are expected to solve the classification problem on movie reviews. Movie reviews have two different sentiments (positive or negative), please train machine learning or deep learning models to classify movie reviews into correct categories (1 for positive 1 and 0 for negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "* Please solve the problems in this notebook using the dataset `IBDM Dataset.csv`.\n",
    "* Important Dates: \n",
    "    * Project Start: Feb 19, Monday\n",
    "    * Project Due: March 7, Thursday midnight\n",
    "* Submission should include a pdf report (at least 4 pages) and code.\n",
    "* There are always last minute issues submitting the project. DO NOT WAIT UNTIL THE LAST MINUTE!\n",
    "\n",
    "**HINT:**\n",
    "* Here are some related tutorials that would be helpful:\n",
    "    * https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/code\n",
    "    * https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "import nltk\n",
    "nltk.data.path.append(\"C:\\\\Users\\\\SUNYLoaner\\\\Desktop\\\\DataProject_1\")  # Adjust the path accordingly\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') \n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming your dataset is in a CSV file, adjust the path accordingly\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common preproccessing steps, feel free to add more preproccessing steps if needed: \n",
    "1. check missing values. \n",
    "2. remove noise and special characters, such as \"\\[[^]]*\\]\", etc.\n",
    "3. transform all words to lower case, \n",
    "4. word tokenization  \n",
    "5. stop words removing and stemming,\n",
    "6. divide the dataset into train set (75%) and test set (25%) with random sampling\n",
    "\n",
    " ......\n",
    "\n",
    "**Hint:**\n",
    "* You may need TfidVectorizer class to convert a collection of raw documents to a matrix of TF-IDF features: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html, \n",
    "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org).\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", data.isnull().sum())\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove noise and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to the 'review' column\n",
    "data['clean_review'] = data['review'].apply(clean_text)\n",
    "\n",
    "# Tokenization and stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply tokenization and stemming to the 'clean_review' column\n",
    "data['processed_review'] = data['clean_review'].apply(tokenize_and_stem)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data['processed_review'], data['sentiment'], test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "# Now, you can train your classification model (e.g., using Naive Bayes)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "report = classification_report(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling\n",
    "\n",
    "* Please use the following models to classify the data:\n",
    "    * Logistic Regression\n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Fully-connected layers, please try different number of hidden layers, different values of \"hidden_layer_sizes\" and \"activation\".\n",
    "    * CNN (please use different number of convolutional layers combined with different number of fully-connected layers, and compare the results).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "pip install keras\n",
    "pip install tensorflow\n",
    "# Import necessary libraries for modeling\n",
    "# Import necessary libraries for modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, train_labels)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(test_labels, lr_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
    "\n",
    "# Results for each model\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression'],\n",
    "    'Accuracy': [lr_accuracy]\n",
    "})\n",
    "\n",
    "# Bar plot for accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "lr_conf_matrix = confusion_matrix(test_labels, lr_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lr_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Linear SVC\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train, train_labels)\n",
    "svc_predictions = svc_model.predict(X_test)\n",
    "svc_accuracy = accuracy_score(test_labels, svc_predictions)\n",
    "print(\"Linear SVC Accuracy:\", svc_accuracy)\n",
    "\n",
    "# Add results for Linear SVC to the DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    'Model': ['Linear SVC'],\n",
    "    'Accuracy': [svc_accuracy]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Bar plot for accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for Linear SVC\n",
    "svc_conf_matrix = confusion_matrix(test_labels, svc_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(svc_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - Linear SVC')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# KNeighbors Classifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, train_labels)\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(test_labels, knn_predictions)\n",
    "print(\"KNeighbors Classifier Accuracy:\", knn_accuracy)\n",
    "\n",
    "# Add results for KNeighbors Classifier to the DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    'Model': ['KNeighbors Classifier'],\n",
    "    'Accuracy': [knn_accuracy]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Bar plot for accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for KNeighbors Classifier\n",
    "knn_conf_matrix = confusion_matrix(test_labels, knn_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(knn_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - KNeighbors Classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for modeling\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# MLP Classifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', max_iter=100)\n",
    "mlp_model.fit(X_train, train_labels)\n",
    "mlp_predictions = mlp_model.predict(X_test)\n",
    "mlp_accuracy = accuracy_score(test_labels, mlp_predictions)\n",
    "print(\"MLP Classifier Accuracy:\", mlp_accuracy)\n",
    "\n",
    "# Add results for MLP Classifier to the DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    'Model': ['MLP Classifier'],\n",
    "    'Accuracy': [mlp_accuracy]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Bar plot for accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for MLP Classifier\n",
    "mlp_conf_matrix = confusion_matrix(test_labels, mlp_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mlp_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - MLP Classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set the NLTK data path\n",
    "nltk.data.path.append(\"C:\\\\Users\\\\SUNYLoaner\\\\Desktop\\\\DataProject_1\")  # Adjust the path accordingly\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", data.isnull().sum())\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data['clean_review'] = data['review'].apply(clean_text)\n",
    "\n",
    "# Tokenization and stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['processed_review'] = data['clean_review'].apply(tokenize_and_stem)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data['processed_review'], data['sentiment'], test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit the number of features\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "X_train_np = X_train.toarray()\n",
    "\n",
    "# Convert 'train_labels' to float32\n",
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "train_labels = train_labels.map(label_mapping).astype('float32')\n",
    "test_labels_numeric = test_labels.map(label_mapping).astype('float32')\n",
    "\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_np, train_labels, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate CNN model on the test set\n",
    "cnn_predictions = (cnn_model.predict(X_test.toarray()) > 0.5).astype(\"int32\")\n",
    "cnn_accuracy = accuracy_score(test_labels_numeric, cnn_predictions)\n",
    "print(\"CNN Model Accuracy:\", cnn_accuracy)\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Add results for CNN to the DataFrame\n",
    "results_df = pd.concat([results_df, pd.DataFrame({\n",
    "    'Model': ['CNN'],\n",
    "    'Accuracy': [cnn_accuracy]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Bar plot for accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Convert string labels to integers for test_labels\n",
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "test_labels_int = test_labels.map(label_mapping)\n",
    "\n",
    "# Convert predictions to integers\n",
    "cnn_predictions_int = cnn_predictions.flatten()\n",
    "\n",
    "# Confusion Matrix for CNN\n",
    "cnn_conf_matrix = confusion_matrix(test_labels_int, cnn_predictions_int)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cnn_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures or tables to present the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "#I have added the results and summary and visualizations of results of each model in the code of execution of models itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "**What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook. Please make sure all the plotted tables and figures are in the notebook. \n",
    "\n",
    "* **PDF Report**: please prepare a report in the PDF form which should be at least 4 pages. The report should includes:\n",
    "\n",
    "  * Data description and exploration.\n",
    "\n",
    "  * Data preproccessing.\n",
    "\n",
    "  * Data modelling.\n",
    "\n",
    "  * What did you find in the data?\n",
    "\n",
    "  * (please include figures or tables in the report, but no source code)\n",
    "  \n",
    "Please compress all the files in a zipped file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
